{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlmjBnVUErl3",
        "outputId": "570086e5-b480-44c4-d201-7313d45a7330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask_cors\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask_cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask_cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask_cors) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask_cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask_cors) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask_cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->flask_cors) (3.0.2)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: flask_cors\n",
            "Successfully installed flask_cors-5.0.1\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.9/171.9 kB\u001b[0m \u001b[31m518.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt_dlp\n",
            "Successfully installed yt_dlp-2025.2.19\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=a47e62388f1ee4633b38039257cca01272623172d014c1f2f1625648d21c350a\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.9 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,676 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,645 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,705 kB]\n",
            "Get:15 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [7,561 B]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,662 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,955 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,824 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n",
            "Fetched 26.4 MB in 3s (7,947 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  ngrok\n",
            "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 9,885 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.20.0 [9,885 kB]\n",
            "Fetched 9,885 kB in 1s (9,457 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ngrok.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../ngrok_3.20.0_amd64.deb ...\n",
            "Unpacking ngrok (3.20.0) ...\n",
            "Setting up ngrok (3.20.0) ...\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install flask_cors\n",
        "!pip install pydub\n",
        "!pip install yt_dlp\n",
        "!pip install pyngrok\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!curl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
        "!ngrok authtoken your-auth-token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouFMFmIFE6IZ",
        "outputId": "361db99d-266d-4a1a-cf24-fd88f0626fd8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://de1e-34-75-120-39.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Feb/2025 20:40:13] \"OPTIONS /summarize HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/-bt_y4Loofg?feature=shared\n",
            "[youtube] -bt_y4Loofg: Downloading webpage\n",
            "[youtube] -bt_y4Loofg: Downloading tv client config\n",
            "[youtube] -bt_y4Loofg: Downloading player 56511309\n",
            "[youtube] -bt_y4Loofg: Downloading tv player API JSON\n",
            "[youtube] -bt_y4Loofg: Downloading ios player API JSON\n",
            "[youtube] -bt_y4Loofg: Downloading m3u8 information\n",
            "[info] -bt_y4Loofg: Downloading 1 format(s): 251\n",
            "[download] Destination: downloaded_audio\n",
            "[download] 100% of    2.60MiB in 00:00:00 at 16.61MiB/s  \n",
            "[ExtractAudio] Destination: downloaded_audio.wav\n",
            "Deleting original file downloaded_audio (pass -k to keep)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Feb/2025 20:40:31] \"POST /summarize HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "777\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [27/Feb/2025 20:40:42] \"OPTIONS /summarize HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/b9eMGE7QtTk?feature=shared\n",
            "[youtube] b9eMGE7QtTk: Downloading webpage\n",
            "[youtube] b9eMGE7QtTk: Downloading tv client config\n",
            "[youtube] b9eMGE7QtTk: Downloading player 56511309\n",
            "[youtube] b9eMGE7QtTk: Downloading tv player API JSON\n",
            "[youtube] b9eMGE7QtTk: Downloading ios player API JSON\n",
            "[youtube] b9eMGE7QtTk: Downloading m3u8 information\n",
            "[info] b9eMGE7QtTk: Downloading 1 format(s): 251\n",
            "[download] Destination: downloaded_audio\n",
            "[download] 100% of   67.13MiB in 00:00:01 at 48.36MiB/s  \n",
            "[ExtractAudio] Destination: downloaded_audio.wav\n",
            "Deleting original file downloaded_audio (pass -k to keep)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from transformers import pipeline\n",
        "from flask_cors import CORS\n",
        "from pydub import AudioSegment\n",
        "import yt_dlp\n",
        "import torch\n",
        "import os\n",
        "import spacy\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict, Counter\n",
        "from IPython.display import display, Image\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "CORS(app)\n",
        "\n",
        "# Initialize device and pipelines\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "asr_pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-base\",\n",
        "    chunk_length_s=30,\n",
        "    device=device,\n",
        "    framework=\"pt\"\n",
        ")\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"sshleifer/distilbart-cnn-12-6\",\n",
        "    device=device,\n",
        "    framework=\"pt\"\n",
        ")\n",
        "\n",
        "# Load NLP model for keyword extraction\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def download_audio(video_url):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio',\n",
        "        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'wav'}],\n",
        "        'outtmpl': 'downloaded_audio'\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([video_url])\n",
        "\n",
        "def split_audio_into_chunks(audio_file_path, chunk_length_ms=60000):\n",
        "    audio = AudioSegment.from_wav(audio_file_path)\n",
        "    total_length = len(audio)\n",
        "    chunks = []\n",
        "    for start in range(0, total_length, chunk_length_ms):\n",
        "        chunk = audio[start:start + chunk_length_ms]\n",
        "        chunk_path = f\"chunk_{start // chunk_length_ms}.wav\"\n",
        "        chunk.export(chunk_path, format=\"wav\")\n",
        "        chunks.append(chunk_path)\n",
        "    return chunks\n",
        "\n",
        "def dynamic_tokenizer_kwargs(input_length,length):\n",
        "    max_length = min(1024, max(10, input_length // length))\n",
        "    min_length = max(10, input_length // (2*length))\n",
        "    return {'truncation': True, 'max_length': max_length, 'min_length': min_length}\n",
        "\n",
        "\n",
        "def summarize_text(text, length):\n",
        "    \"\"\"Summarizes the given text after cleaning and chunking.\"\"\"\n",
        "    cleaned_sentences = remove_outliers(text)  # Remove unnecessary sentences\n",
        "    full_transcription = \" \".join(cleaned_sentences)\n",
        "\n",
        "    summaries = []\n",
        "    chunk_size = 300  # Summarization works best on ~300 words\n",
        "    words = full_transcription.split()\n",
        "\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        input_length = len(chunk.split())\n",
        "        tokenizer_kwargs = dynamic_tokenizer_kwargs(input_length, length)\n",
        "        summary = summarizer(chunk, do_sample=False, **tokenizer_kwargs)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    return \" \".join(summaries), full_transcription\n",
        "\n",
        "\n",
        "def generate_mind_map(summary_text):\n",
        "    doc = nlp(summary_text)\n",
        "    nodes = []\n",
        "    links = []\n",
        "    keyword_map = defaultdict(set)\n",
        "\n",
        "    main_topics = set()\n",
        "\n",
        "    # Extract key topics and relationships\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"]:  # Focus on nouns & proper nouns\n",
        "            parent = token.head.text\n",
        "            if parent != token.text:  # Avoid self-links\n",
        "                keyword_map[parent].add(token.text)\n",
        "                main_topics.add(parent)\n",
        "\n",
        "    # Build nodes and links\n",
        "    seen_nodes = set()\n",
        "    for main_topic in main_topics:\n",
        "        main_id = f\"node-{main_topic}\"\n",
        "        if main_id not in seen_nodes:\n",
        "            nodes.append({\"id\": main_id, \"name\": main_topic, \"type\": \"heading\"})\n",
        "            seen_nodes.add(main_id)\n",
        "\n",
        "        for sub in keyword_map[main_topic]:\n",
        "            sub_id = f\"node-{sub}\"\n",
        "            if sub_id not in seen_nodes:\n",
        "                nodes.append({\"id\": sub_id, \"name\": sub, \"type\": \"subtopic\"})\n",
        "                seen_nodes.add(sub_id)\n",
        "            links.append({\"source\": main_id, \"target\": sub_id})\n",
        "\n",
        "    return {\"nodes\": nodes, \"links\": links}\n",
        "\n",
        "def extract_topic(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract nouns and proper nouns\n",
        "    words = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop]\n",
        "\n",
        "    # Count occurrences of each noun\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    # Prioritize words appearing in the first sentence(s)\n",
        "    first_sentences = list(doc.sents)[:2]  # Look at the first two sentences\n",
        "    first_mentions = [token.text for sent in first_sentences for token in sent if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
        "\n",
        "    # Give extra weight to early mentions\n",
        "    for word in first_mentions:\n",
        "        word_freq[word] += 3  # Boost early mentions\n",
        "\n",
        "    # Select the most frequently mentioned word\n",
        "    main_topic = word_freq.most_common(1)\n",
        "\n",
        "    return main_topic[0][0] if main_topic else \"Unknown Topic\"\n",
        "\n",
        "def extract_sentences(summary):\n",
        "    doc = nlp(summary)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "def extract_main_topics(text, top_n=10):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON', 'GPE', 'LOC', 'PRODUCT']]\n",
        "    entity_freq = Counter(entities)\n",
        "\n",
        "    if entity_freq:\n",
        "        main_topics = [topic for topic, _ in entity_freq.most_common(top_n)]\n",
        "    else:\n",
        "        words = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
        "        word_freq = Counter(words)\n",
        "        main_topics = [word for word, _ in word_freq.most_common(top_n)]\n",
        "\n",
        "    return main_topics\n",
        "\n",
        "def find_sentence_relations(keywords, sentences, max_sentences=1):\n",
        "    keyword_sentences = defaultdict(list)\n",
        "    assigned_sentences = set()  # To keep track of assigned sentences\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.split()) > 10 and sentence not in assigned_sentences:\n",
        "                if keyword.lower() in sentence.lower():\n",
        "                    if len(keyword_sentences[keyword]) < max_sentences:\n",
        "                        keyword_sentences[keyword].append(sentence)\n",
        "                        assigned_sentences.add(sentence)  # Mark sentence as used\n",
        "                        break  # Stop after assigning the sentence to the keyword\n",
        "\n",
        "    return keyword_sentences\n",
        "\n",
        "def get_video_title(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    return soup.title.string.replace(\"- YouTube\", \"\").strip()\n",
        "\n",
        "def extract_most_frequent_title_word(title, text):\n",
        "    doc = nlp(text.lower())  # Convert to lowercase for consistency\n",
        "    words = [token.text for token in doc if token.is_alpha and not token.is_stop]  # Remove non-alphabetic words and stop words\n",
        "\n",
        "    # Tokenize title words\n",
        "    title_words = title.lower().split()\n",
        "\n",
        "    # Count occurrences of title words in transcription\n",
        "    word_freq = Counter(words)\n",
        "    common_words = {word: word_freq[word] for word in title_words if word in word_freq}\n",
        "\n",
        "    # Find the most frequent word\n",
        "    return max(common_words, key=common_words.get, default=\"Unknown Title\")\n",
        "\n",
        "outro_patterns = [\n",
        "    r\"thanks for watching\", r\"see you in the next video\", r\"don't forget to subscribe\",\n",
        "    r\"hit the (like|subscribe) button\", r\"leave a comment\", r\"hope you enjoyed\",\n",
        "    r\"follow for more\", r\"stay tuned\", r\"this was all about\", r\"let me know in the comments\",\n",
        "    r\"Welcome to this video\", r\"let's wrap up\", r\"in 100 seconds\", r\"this has been\", r\"for watching\", r\"video\"\n",
        "]\n",
        "\n",
        "def remove_outliers(text):\n",
        "    \"\"\" Removes unwanted sentences that match outro patterns. \"\"\"\n",
        "    sentences = list(nlp(text).sents)  # Segment into sentences\n",
        "    filtered_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text.strip()\n",
        "        sentence_lower = sentence_text.lower()\n",
        "\n",
        "        # Skip sentences matching engagement/outro phrases\n",
        "        if any(re.search(pattern, sentence_lower) for pattern in outro_patterns):\n",
        "            continue\n",
        "\n",
        "        # Skip very short sentences (likely irrelevant)\n",
        "        if len(sentence_text.split()) < 4:\n",
        "            continue\n",
        "\n",
        "        filtered_sentences.append(sentence_text)\n",
        "\n",
        "    return filtered_sentences\n",
        "\n",
        "def translate_text(text, target_language=\"es\"):\n",
        "    try:\n",
        "        result = translator.translate(text, dest=target_language)\n",
        "        return result.text\n",
        "    except Exception as e:\n",
        "        print(\"Translation Error:\", str(e))\n",
        "        return text  # Return the original text if translation fails\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"Hello, Flask is running on Colab!\"\n",
        "\n",
        "@app.route(\"/summarize\", methods=[\"POST\"])\n",
        "def summarize_video():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        video_url = data.get(\"video_url\")\n",
        "        if not video_url:\n",
        "            return jsonify({\"error\": \"Missing video_url\"}), 400\n",
        "\n",
        "        target_language = data.get(\"lang\")\n",
        "        if not target_language:\n",
        "            return jsonify({\"error\": \"Missing target language\"}), 400\n",
        "\n",
        "        # Determine summary length\n",
        "        summary_length = data.get(\"summary_length\", \"Normal\")\n",
        "        length = {\"Short\": 3, \"Normal\": 2, \"Long\": 1}.get(summary_length, 2)\n",
        "\n",
        "        # Download and split audio\n",
        "        download_audio(video_url)\n",
        "        audio_file_path = \"downloaded_audio.wav\"\n",
        "        chunks = split_audio_into_chunks(audio_file_path)\n",
        "\n",
        "        # Perform transcription\n",
        "        transcription = \" \".join([asr_pipe(chunk)[\"text\"] for chunk in chunks])\n",
        "\n",
        "        # Summarization\n",
        "        full_summary, full_transcription = summarize_text(transcription, length)\n",
        "\n",
        "        points = extract_sentences(full_transcription)\n",
        "        text_data = \"\\n\".join(points)\n",
        "\n",
        "        # Extract main topics (keywords)\n",
        "        main_topics = extract_main_topics(text_data)\n",
        "\n",
        "        # Determine central topic\n",
        "        video_title = get_video_title(video_url)\n",
        "        most_frequent_word = extract_most_frequent_title_word(video_title, full_transcription)\n",
        "\n",
        "        # Find sentence relations for mind map\n",
        "        keyword_sentences = find_sentence_relations(main_topics, points)\n",
        "\n",
        "        # Translation (if needed)\n",
        "        if target_language != \"en\":\n",
        "            full_summary = translate_text(full_summary, target_language)\n",
        "            full_transcription = translate_text(full_transcription, target_language)\n",
        "\n",
        "        # Cleanup\n",
        "        os.remove(audio_file_path)\n",
        "        for chunk in chunks:\n",
        "            os.remove(chunk)\n",
        "\n",
        "        print(len(full_summary))\n",
        "\n",
        "\n",
        "        return jsonify({\n",
        "            \"summary\": full_summary,\n",
        "            \"transcription\": full_transcription,\n",
        "            \"mind_map\": keyword_sentences,\n",
        "            \"central\": most_frequent_word\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    port = 5000\n",
        "    public_url = ngrok.connect(port).public_url\n",
        "    print(\"Public URL:\", public_url)\n",
        "    app.run(host=\"0.0.0.0\",port=port)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
