{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [27/Feb/2025 13:55:44] \"OPTIONS /summarize HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received Data: {'video_url': 'https://youtu.be/-bt_y4Loofg?feature=shared', 'lang': 'hi'}\n",
      "Target Language: hi\n",
      "[youtube] Extracting URL: https://youtu.be/-bt_y4Loofg?feature=shared\n",
      "[youtube] -bt_y4Loofg: Downloading webpage\n",
      "[youtube] -bt_y4Loofg: Downloading ios player API JSON\n",
      "[youtube] -bt_y4Loofg: Downloading mweb player API JSON\n",
      "[youtube] -bt_y4Loofg: Downloading player 56511309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/56511309/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] -bt_y4Loofg: nsig extraction failed: Some formats may be missing\n",
      "         n = H15IjmFtA19FkBlLwUdd ; player = https://www.youtube.com/s/player/56511309/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] Falling back to generic n function search\n",
      "         player = https://www.youtube.com/s/player/56511309/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] -bt_y4Loofg: nsig extraction failed: Some formats may be missing\n",
      "         n = BcRGbnX0_T0EVftxOJLd ; player = https://www.youtube.com/s/player/56511309/player_ias.vflset/en_US/base.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] -bt_y4Loofg: Downloading m3u8 information\n",
      "[info] -bt_y4Loofg: Downloading 1 format(s): 251\n",
      "[download] Destination: downloaded_audio\n",
      "[download] 100% of    2.60MiB in 00:00:03 at 748.63KiB/s \n",
      "[ExtractAudio] Destination: downloaded_audio.wav\n",
      "Deleting original file downloaded_audio (pass -k to keep)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [27/Feb/2025 13:57:10] \"POST /summarize HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import pipeline\n",
    "from flask_cors import CORS\n",
    "from pydub import AudioSegment\n",
    "import yt_dlp\n",
    "import torch\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize device and pipelines\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-base\",\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"sshleifer/distilbart-cnn-12-6\",\n",
    "    device=device,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# Load NLP model for keyword extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def download_audio(video_url):\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio',\n",
    "        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'wav'}],\n",
    "        'outtmpl': 'downloaded_audio'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_url])\n",
    "\n",
    "def split_audio_into_chunks(audio_file_path, chunk_length_ms=60000):\n",
    "    audio = AudioSegment.from_wav(audio_file_path)\n",
    "    total_length = len(audio)\n",
    "    chunks = []\n",
    "    for start in range(0, total_length, chunk_length_ms):\n",
    "        chunk = audio[start:start + chunk_length_ms]\n",
    "        chunk_path = f\"chunk_{start // chunk_length_ms}.wav\"\n",
    "        chunk.export(chunk_path, format=\"wav\")\n",
    "        chunks.append(chunk_path)\n",
    "    return chunks\n",
    "\n",
    "def dynamic_tokenizer_kwargs(input_length):\n",
    "    max_length = min(1024, max(10, input_length // 2))\n",
    "    min_length = max(10, input_length // 4)\n",
    "    return {'truncation': True, 'max_length': max_length, 'min_length': min_length}\n",
    "\n",
    "def process_audio_chunks(audio_chunks):\n",
    "    texts = [asr_pipe(chunk)[\"text\"] for chunk in audio_chunks]\n",
    "    summaries = []\n",
    "    for text in texts:\n",
    "        input_length = len(text.split())\n",
    "        tokenizer_kwargs = dynamic_tokenizer_kwargs(input_length)\n",
    "        summary = summarizer(text, do_sample=False, **tokenizer_kwargs)\n",
    "        summaries.append(summary[0]['summary_text'])\n",
    "    return summaries\n",
    "\n",
    "def generate_mind_map(summary_text):\n",
    "    doc = nlp(summary_text)\n",
    "    nodes = []\n",
    "    links = []\n",
    "    keyword_map = defaultdict(set)\n",
    "    \n",
    "    main_topics = set()\n",
    "    \n",
    "    # Extract key topics and relationships\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\"]:  # Focus on nouns & proper nouns\n",
    "            parent = token.head.text\n",
    "            if parent != token.text:  # Avoid self-links\n",
    "                keyword_map[parent].add(token.text)\n",
    "                main_topics.add(parent)\n",
    "\n",
    "    # Build nodes and links\n",
    "    seen_nodes = set()\n",
    "    for main_topic in main_topics:\n",
    "        main_id = f\"node-{main_topic}\"\n",
    "        if main_id not in seen_nodes:\n",
    "            nodes.append({\"id\": main_id, \"name\": main_topic, \"type\": \"heading\"})\n",
    "            seen_nodes.add(main_id)\n",
    "\n",
    "        for sub in keyword_map[main_topic]:\n",
    "            sub_id = f\"node-{sub}\"\n",
    "            if sub_id not in seen_nodes:\n",
    "                nodes.append({\"id\": sub_id, \"name\": sub, \"type\": \"subtopic\"})\n",
    "                seen_nodes.add(sub_id)\n",
    "            links.append({\"source\": main_id, \"target\": sub_id})\n",
    "\n",
    "    return {\"nodes\": nodes, \"links\": links}\n",
    "\n",
    "def extract_topic(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract nouns and proper nouns\n",
    "    words = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop]\n",
    "\n",
    "    # Count occurrences of each noun\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Prioritize words appearing in the first sentence(s)\n",
    "    first_sentences = list(doc.sents)[:2]  # Look at the first two sentences\n",
    "    first_mentions = [token.text for sent in first_sentences for token in sent if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "\n",
    "    # Give extra weight to early mentions\n",
    "    for word in first_mentions:\n",
    "        word_freq[word] += 3  # Boost early mentions\n",
    "\n",
    "    # Select the most frequently mentioned word\n",
    "    main_topic = word_freq.most_common(1)\n",
    "\n",
    "    return main_topic[0][0] if main_topic else \"Unknown Topic\"\n",
    "\n",
    "def extract_sentences(summary):\n",
    "    doc = nlp(summary)\n",
    "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "def extract_main_topics(text, top_n=10):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON', 'GPE', 'LOC', 'PRODUCT']]\n",
    "    entity_freq = Counter(entities)\n",
    "\n",
    "    if entity_freq:\n",
    "        main_topics = [topic for topic, _ in entity_freq.most_common(top_n)]\n",
    "    else:\n",
    "        words = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "        word_freq = Counter(words)\n",
    "        main_topics = [word for word, _ in word_freq.most_common(top_n)]\n",
    "\n",
    "    return main_topics\n",
    "\n",
    "def find_sentence_relations(keywords, sentences, max_sentences=1):\n",
    "    keyword_sentences = defaultdict(list)\n",
    "    assigned_sentences = set()  # To keep track of assigned sentences\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        for sentence in sentences:\n",
    "            if len(sentence.split()) > 10 and sentence not in assigned_sentences:\n",
    "                if keyword.lower() in sentence.lower():\n",
    "                    if len(keyword_sentences[keyword]) < max_sentences:\n",
    "                        keyword_sentences[keyword].append(sentence)\n",
    "                        assigned_sentences.add(sentence)  # Mark sentence as used\n",
    "                        break  # Stop after assigning the sentence to the keyword\n",
    "    \n",
    "    return keyword_sentences\n",
    "\n",
    "def get_video_title(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup.title.string.replace(\"- YouTube\", \"\").strip()\n",
    "\n",
    "def extract_most_frequent_title_word(title, text):\n",
    "    doc = nlp(text.lower())  # Convert to lowercase for consistency\n",
    "    words = [token.text for token in doc if token.is_alpha and not token.is_stop]  # Remove non-alphabetic words and stop words\n",
    "\n",
    "    # Tokenize title words\n",
    "    title_words = title.lower().split()\n",
    "\n",
    "    # Count occurrences of title words in transcription\n",
    "    word_freq = Counter(words)\n",
    "    common_words = {word: word_freq[word] for word in title_words if word in word_freq}\n",
    "\n",
    "    # Find the most frequent word\n",
    "    return max(common_words, key=common_words.get, default=\"Unknown Title\")\n",
    "\n",
    "outro_patterns = [\n",
    "    r\"thanks for watching\", r\"see you in the next video\", r\"don't forget to subscribe\",\n",
    "    r\"hit the (like|subscribe) button\", r\"leave a comment\", r\"hope you enjoyed\",\n",
    "    r\"follow for more\", r\"stay tuned\", r\"this was all about\", r\"let me know in the comments\",\n",
    "    r\"Welcome to this video\", r\"let's wrap up\", r\"in 100 seconds\", r\"this has been\", r\"for watching\"\n",
    "]\n",
    "\n",
    "def remove_outliers(text):\n",
    "    \"\"\" Removes unwanted sentences that match outro patterns. \"\"\"\n",
    "    sentences = list(nlp(text).sents)  # Segment into sentences\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_text = sentence.text.strip()\n",
    "        sentence_lower = sentence_text.lower()\n",
    "\n",
    "        # Skip sentences matching engagement/outro phrases\n",
    "        if any(re.search(pattern, sentence_lower) for pattern in outro_patterns):\n",
    "            continue\n",
    "\n",
    "        # Skip very short sentences (likely irrelevant)\n",
    "        if len(sentence_text.split()) < 4:\n",
    "            continue\n",
    "\n",
    "        filtered_sentences.append(sentence_text)\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "def translate_text(text, target_language=\"es\"):\n",
    "    try:\n",
    "        result = translator.translate(text, dest=target_language)\n",
    "        return result.text\n",
    "    except Exception as e:\n",
    "        print(\"Translation Error:\", str(e))\n",
    "        return text  # Return the original text if translation fails\n",
    "\n",
    "\n",
    "@app.route(\"/summarize\", methods=[\"POST\"])\n",
    "def summarize_video():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        print(\"Received Data:\", data)\n",
    "        video_url = data.get(\"video_url\")\n",
    "        if not video_url:\n",
    "            return jsonify({\"error\": \"Missing video_url\"}), 400\n",
    "        \n",
    "        target_language = data.get(\"lang\")\n",
    "        if not target_language:\n",
    "            return jsonify({\"error\": \"Missing target language\"}), 400\n",
    "\n",
    "        print(\"Target Language:\", target_language)\n",
    "\n",
    "        download_audio(video_url)\n",
    "        audio_file_path = \"downloaded_audio.wav\"\n",
    "        chunks = split_audio_into_chunks(audio_file_path)\n",
    "\n",
    "        # Store full transcription text\n",
    "        transcription = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            text = asr_pipe(chunk)[\"text\"]\n",
    "            transcription.append(text)  # Store each chunk\n",
    "        transcription = \" \".join(transcription)  # Join all transcriptions\n",
    "\n",
    "\n",
    "\n",
    "        full_transcription = []\n",
    "\n",
    "        cleaned_sentences = remove_outliers(transcription)\n",
    "\n",
    "        for sent in cleaned_sentences:\n",
    "            full_transcription.append(sent)\n",
    "\n",
    "        full_transcription = \" \".join(full_transcription)\n",
    "\n",
    "\n",
    "        # Summarization\n",
    "        all_summaries = []\n",
    "        for text in full_transcription.split(\"\\n\"):\n",
    "            if text.strip():\n",
    "                input_length = len(text.split())\n",
    "                tokenizer_kwargs = dynamic_tokenizer_kwargs(input_length)\n",
    "                summary = summarizer(text, do_sample=False, **tokenizer_kwargs)\n",
    "                all_summaries.append(summary[0]['summary_text'])\n",
    "\n",
    "        # Cleanup\n",
    "        os.remove(audio_file_path)\n",
    "        for chunk in chunks:\n",
    "            os.remove(chunk)\n",
    "\n",
    "        full_summary = \" \".join(all_summaries)\n",
    "        points = extract_sentences(full_transcription)\n",
    "        text_data = \"\\n\".join(points)\n",
    "\n",
    "        # Extract main topics (keywords)\n",
    "        main_topics = extract_main_topics(text_data)\n",
    "\n",
    "        #central topic finding\n",
    "        video_title = get_video_title(video_url)\n",
    "        most_frequent_word = extract_most_frequent_title_word(video_title, full_transcription)\n",
    "\n",
    "\n",
    "        # Find relationships between keywords and sentences\n",
    "        keyword_sentences = find_sentence_relations(main_topics, points)\n",
    "\n",
    "\n",
    "\n",
    "        translated_summary = translate_text(full_summary, target_language)\n",
    "        translated_transcription = translate_text(text_data, target_language)\n",
    "       \n",
    "\n",
    "        return jsonify({\n",
    "            \"summary\": translated_summary,\n",
    "            \"transcription\": translated_transcription,  # Store the full transcription\n",
    "            \"mind_map\": keyword_sentences,\n",
    "            \"central\": most_frequent_word\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\victus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 524.3 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 524.3 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 0.8/1.3 MB 621.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.0/1.3 MB 636.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.0/1.3 MB 636.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 659.6 kB/s eta 0:00:00\n",
      "Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17457 sha256=a32e3e2ccd62fad42650ca121db0eaa08e3f90eb67353e109d93810f492b5214\n",
      "  Stored in directory: c:\\users\\victus\\appdata\\local\\pip\\cache\\wheels\\95\\0f\\04\\b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: hyperframe\n",
      "    Found existing installation: hyperframe 6.1.0\n",
      "    Uninstalling hyperframe-6.1.0:\n",
      "      Successfully uninstalled hyperframe-6.1.0\n",
      "  Attempting uninstall: hpack\n",
      "    Found existing installation: hpack 4.1.0\n",
      "    Uninstalling hpack-4.1.0:\n",
      "      Successfully uninstalled hpack-4.1.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: h2\n",
      "    Found existing installation: h2 4.2.0\n",
      "    Uninstalling h2-4.2.0:\n",
      "      Successfully uninstalled h2-4.2.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.7\n",
      "    Uninstalling httpcore-1.0.7:\n",
      "      Successfully uninstalled httpcore-1.0.7\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 4.0.2\n",
      "    Uninstalling googletrans-4.0.2:\n",
      "      Successfully uninstalled googletrans-4.0.2\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install flask_cors\n",
    "!pip install pydub\n",
    "!pip install yt_dlp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
