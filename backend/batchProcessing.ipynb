{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UxMWotQrE67",
        "outputId": "08b3e345-fb99-47de-ec6c-889738d8ff9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask_cors in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask_cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask_cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask_cors) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask_cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask_cors) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask_cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->flask_cors) (3.0.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: yt_dlp in /usr/local/lib/python3.11/dist-packages (2025.2.19)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.11/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.11/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.11/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ngrok is already the newest version (3.20.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!pip install flask_cors\n",
        "!pip install pydub\n",
        "!pip install yt_dlp\n",
        "!pip install pyngrok\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install datasets\n",
        "!curl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
        "!ngrok authtoken your-auth-token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPZqrN80D4np",
        "outputId": "7ffaf89e-0cb4-4033-f7eb-9c71695b6923"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://bfb8-35-185-199-89.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Feb/2025 19:25:26] \"OPTIONS /summarize HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/b9eMGE7QtTk?feature=shared\n",
            "[youtube] b9eMGE7QtTk: Downloading webpage\n",
            "[youtube] b9eMGE7QtTk: Downloading tv client config\n",
            "[youtube] b9eMGE7QtTk: Downloading player 56511309\n",
            "[youtube] b9eMGE7QtTk: Downloading tv player API JSON\n",
            "[youtube] b9eMGE7QtTk: Downloading ios player API JSON\n",
            "[youtube] b9eMGE7QtTk: Downloading m3u8 information\n",
            "[info] b9eMGE7QtTk: Downloading 1 format(s): 251\n",
            "[download] Destination: downloaded_audio\n",
            "[download] 100% of   67.13MiB in 00:00:01 at 48.04MiB/s  \n",
            "[ExtractAudio] Destination: downloaded_audio.wav\n",
            "Deleting original file downloaded_audio (pass -k to keep)\n",
            "15095\n",
            "Using batch size: 48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "INFO:werkzeug:127.0.0.1 - - [27/Feb/2025 19:27:34] \"POST /summarize HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14670\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "# from flask_ngrok import run_with_ngrok\n",
        "from pyngrok import ngrok\n",
        "from transformers import pipeline\n",
        "from flask_cors import CORS\n",
        "from pydub import AudioSegment\n",
        "import yt_dlp\n",
        "import torch\n",
        "import os\n",
        "import spacy\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict, Counter\n",
        "from IPython.display import display, Image\n",
        "from datasets import Dataset\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "app = Flask(__name__)\n",
        "# run_with_ngrok(app)  # Start ngrok when the app runs\n",
        "\n",
        "CORS(app)\n",
        "\n",
        "# Initialize device and pipelines\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "asr_pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-base\",\n",
        "    chunk_length_s=30,\n",
        "    device=device,\n",
        "    framework=\"pt\"\n",
        ")\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"sshleifer/distilbart-cnn-12-6\",\n",
        "    device=device,\n",
        "    framework=\"pt\"\n",
        ")\n",
        "\n",
        "# Load NLP model for keyword extraction\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def download_audio(video_url):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio',\n",
        "        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'wav'}],\n",
        "        'outtmpl': 'downloaded_audio'\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([video_url])\n",
        "\n",
        "\n",
        "def split_audio_into_chunks(audio_file_path, chunk_length_ms=60000, overlap_ms=5000):\n",
        "    \"\"\"\n",
        "    Splits an audio file into overlapping chunks to avoid sentence cut-offs.\n",
        "\n",
        "    Parameters:\n",
        "    - audio_file_path (str): Path to the input audio file (WAV format).\n",
        "    - chunk_length_ms (int): Length of each chunk in milliseconds (default: 60 seconds).\n",
        "    - overlap_ms (int): Overlapping duration between chunks in milliseconds (default: 5 seconds).\n",
        "\n",
        "    Returns:\n",
        "    - List of chunk file paths.\n",
        "    \"\"\"\n",
        "    audio = AudioSegment.from_wav(audio_file_path)\n",
        "    total_length = len(audio)\n",
        "    chunks = []\n",
        "\n",
        "    start = 0\n",
        "    chunk_id = 0\n",
        "\n",
        "    while start < total_length:\n",
        "        # Define the end position (ensure it doesn't exceed total length)\n",
        "        end = min(start + chunk_length_ms, total_length)\n",
        "\n",
        "        # Extract the chunk with overlap\n",
        "        chunk = audio[start:end]\n",
        "\n",
        "        # Export chunk to file\n",
        "        chunk_path = f\"chunk_{chunk_id}.wav\"\n",
        "        chunk.export(chunk_path, format=\"wav\")\n",
        "        chunks.append(chunk_path)\n",
        "\n",
        "        # Move the start position, keeping overlap\n",
        "        start += (chunk_length_ms - overlap_ms)\n",
        "        chunk_id += 1\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def dynamic_tokenizer_kwargs(input_length,length):\n",
        "    max_length = min(1024, max(10, input_length // length))\n",
        "    min_length = max(10, input_length // (2*length))\n",
        "    return {'truncation': True, 'max_length': max_length, 'min_length': min_length}\n",
        "\n",
        "\n",
        "def summarize_text(text, length):\n",
        "    \"\"\"Summarizes the given text after cleaning and chunking.\"\"\"\n",
        "    cleaned_sentences = remove_outliers(text)  # Remove unnecessary sentences\n",
        "    full_transcription = \" \".join(cleaned_sentences)\n",
        "\n",
        "    summaries = []\n",
        "    chunk_size = 300  # Summarization works best on ~300 words\n",
        "    words = full_transcription.split()\n",
        "\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        input_length = len(chunk.split())\n",
        "        tokenizer_kwargs = dynamic_tokenizer_kwargs(input_length, length)\n",
        "        summary = summarizer(chunk, do_sample=False, **tokenizer_kwargs)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    return \" \".join(summaries), full_transcription\n",
        "\n",
        "\n",
        "def generate_mind_map(summary_text):\n",
        "    doc = nlp(summary_text)\n",
        "    nodes = []\n",
        "    links = []\n",
        "    keyword_map = defaultdict(set)\n",
        "\n",
        "    main_topics = set()\n",
        "\n",
        "    # Extract key topics and relationships\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"]:  # Focus on nouns & proper nouns\n",
        "            parent = token.head.text\n",
        "            if parent != token.text:  # Avoid self-links\n",
        "                keyword_map[parent].add(token.text)\n",
        "                main_topics.add(parent)\n",
        "\n",
        "    # Build nodes and links\n",
        "    seen_nodes = set()\n",
        "    for main_topic in main_topics:\n",
        "        main_id = f\"node-{main_topic}\"\n",
        "        if main_id not in seen_nodes:\n",
        "            nodes.append({\"id\": main_id, \"name\": main_topic, \"type\": \"heading\"})\n",
        "            seen_nodes.add(main_id)\n",
        "\n",
        "        for sub in keyword_map[main_topic]:\n",
        "            sub_id = f\"node-{sub}\"\n",
        "            if sub_id not in seen_nodes:\n",
        "                nodes.append({\"id\": sub_id, \"name\": sub, \"type\": \"subtopic\"})\n",
        "                seen_nodes.add(sub_id)\n",
        "            links.append({\"source\": main_id, \"target\": sub_id})\n",
        "\n",
        "    return {\"nodes\": nodes, \"links\": links}\n",
        "\n",
        "def extract_topic(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract nouns and proper nouns\n",
        "    words = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop]\n",
        "\n",
        "    # Count occurrences of each noun\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    # Prioritize words appearing in the first sentence(s)\n",
        "    first_sentences = list(doc.sents)[:2]  # Look at the first two sentences\n",
        "    first_mentions = [token.text for sent in first_sentences for token in sent if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
        "\n",
        "    # Give extra weight to early mentions\n",
        "    for word in first_mentions:\n",
        "        word_freq[word] += 3  # Boost early mentions\n",
        "\n",
        "    # Select the most frequently mentioned word\n",
        "    main_topic = word_freq.most_common(1)\n",
        "\n",
        "    return main_topic[0][0] if main_topic else \"Unknown Topic\"\n",
        "\n",
        "def extract_sentences(summary):\n",
        "    doc = nlp(summary)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "def extract_main_topics(text, top_n=10):\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON', 'GPE', 'LOC', 'PRODUCT']]\n",
        "    entity_freq = Counter(entities)\n",
        "\n",
        "    if entity_freq:\n",
        "        main_topics = [topic for topic, _ in entity_freq.most_common(top_n)]\n",
        "    else:\n",
        "        words = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
        "        word_freq = Counter(words)\n",
        "        main_topics = [word for word, _ in word_freq.most_common(top_n)]\n",
        "\n",
        "    return main_topics\n",
        "\n",
        "def find_sentence_relations(keywords, sentences, max_sentences=1):\n",
        "    keyword_sentences = defaultdict(list)\n",
        "    assigned_sentences = set()  # To keep track of assigned sentences\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.split()) > 10 and sentence not in assigned_sentences:\n",
        "                if keyword.lower() in sentence.lower():\n",
        "                    if len(keyword_sentences[keyword]) < max_sentences:\n",
        "                        keyword_sentences[keyword].append(sentence)\n",
        "                        assigned_sentences.add(sentence)  # Mark sentence as used\n",
        "                        break  # Stop after assigning the sentence to the keyword\n",
        "\n",
        "    return keyword_sentences\n",
        "\n",
        "def get_video_title(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    return soup.title.string.replace(\"- YouTube\", \"\").strip()\n",
        "\n",
        "def extract_most_frequent_title_word(title, text):\n",
        "    doc = nlp(text.lower())  # Convert to lowercase for consistency\n",
        "    words = [token.text for token in doc if token.is_alpha and not token.is_stop]  # Remove non-alphabetic words and stop words\n",
        "\n",
        "    # Tokenize title words\n",
        "    title_words = title.lower().split()\n",
        "\n",
        "    # Count occurrences of title words in transcription\n",
        "    word_freq = Counter(words)\n",
        "    common_words = {word: word_freq[word] for word in title_words if word in word_freq}\n",
        "\n",
        "    # Find the most frequent word\n",
        "    return max(common_words, key=common_words.get, default=\"Unknown Title\")\n",
        "\n",
        "outro_patterns = [\n",
        "    r\"thanks for watching\", r\"see you in the next video\", r\"don't forget to subscribe\",\n",
        "    r\"hit the (like|subscribe) button\", r\"leave a comment\", r\"hope you enjoyed\",\n",
        "    r\"follow for more\", r\"stay tuned\", r\"this was all about\", r\"let me know in the comments\",\n",
        "    r\"Welcome to this video\", r\"let's wrap up\", r\"in 100 seconds\", r\"this has been\", r\"for watching\", r\"video\"\n",
        "]\n",
        "\n",
        "def remove_outliers(text):\n",
        "    \"\"\" Removes unwanted sentences that match outro patterns. \"\"\"\n",
        "    sentences = list(nlp(text).sents)  # Segment into sentences\n",
        "    filtered_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text.strip()\n",
        "        sentence_lower = sentence_text.lower()\n",
        "\n",
        "        # Skip sentences matching engagement/outro phrases\n",
        "        if any(re.search(pattern, sentence_lower) for pattern in outro_patterns):\n",
        "            continue\n",
        "\n",
        "        # Skip very short sentences (likely irrelevant)\n",
        "        if len(sentence_text.split()) < 4:\n",
        "            continue\n",
        "\n",
        "        filtered_sentences.append(sentence_text)\n",
        "\n",
        "    return filtered_sentences\n",
        "\n",
        "def translate_text(text, target_language=\"es\"):\n",
        "    try:\n",
        "        result = translator.translate(text, dest=target_language)\n",
        "        return result.text\n",
        "    except Exception as e:\n",
        "        print(\"Translation Error:\", str(e))\n",
        "        return text  # Return the original text if translation fails\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"Hello, Flask is running on Colab!\"\n",
        "\n",
        "@app.route(\"/summarize\", methods=[\"POST\"])\n",
        "def summarize_video():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        video_url = data.get(\"video_url\")\n",
        "        if not video_url:\n",
        "            return jsonify({\"error\": \"Missing video_url\"}), 400\n",
        "\n",
        "        target_language = data.get(\"lang\")\n",
        "        if not target_language:\n",
        "            return jsonify({\"error\": \"Missing target language\"}), 400\n",
        "\n",
        "        # Determine summary length\n",
        "        summary_length = data.get(\"summary_length\", \"Normal\")\n",
        "        length = {\"Short\": 3, \"Normal\": 2, \"Long\": 1}.get(summary_length, 2)\n",
        "\n",
        "        # Download and split audio\n",
        "        download_audio(video_url)\n",
        "        audio_file_path = \"downloaded_audio.wav\"\n",
        "        chunks = split_audio_into_chunks(audio_file_path, chunk_length_ms=60000, overlap_ms=5000)\n",
        "\n",
        "        # Perform transcription\n",
        "        dataset = Dataset.from_dict({\"audio\": chunks})\n",
        "\n",
        "        # Process multiple audio chunks in a single batch\n",
        "        def get_optimal_batch_size():\n",
        "            total_vram = torch.cuda.get_device_properties(0).total_memory // (1024 ** 2)  # Get VRAM in MB\n",
        "            print(total_vram)\n",
        "            if total_vram >= 14000:  # T4 has 16GB\n",
        "                return 48  # Safe batch size\n",
        "            elif total_vram >= 12000:\n",
        "                return 32\n",
        "            else:\n",
        "                return 24  # Safe fallback\n",
        "\n",
        "        batch_size = get_optimal_batch_size()\n",
        "        print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "        transcriptions = asr_pipe(dataset[\"audio\"], batch_size=batch_size)\n",
        "\n",
        "        # Extract text and join into a single transcription\n",
        "        transcription = \" \".join([t[\"text\"] for t in transcriptions])\n",
        "\n",
        "        # Summarization\n",
        "        full_summary, full_transcription = summarize_text(transcription, length)\n",
        "\n",
        "        points = extract_sentences(full_transcription)\n",
        "        text_data = \"\\n\".join(points)\n",
        "        # -------------------------------------------\n",
        "        \n",
        "        #---------------------------------------------------\n",
        "        # Extract main topics (keywords)\n",
        "        main_topics = extract_main_topics(text_data)\n",
        "\n",
        "        # Determine central topic\n",
        "        video_title = get_video_title(video_url)\n",
        "        most_frequent_word = extract_most_frequent_title_word(video_title, full_transcription)\n",
        "\n",
        "        # Find sentence relations for mind map\n",
        "        keyword_sentences = find_sentence_relations(main_topics, points)\n",
        "\n",
        "        # Translation (if needed)\n",
        "        if target_language != \"en\":\n",
        "            full_summary = translate_text(full_summary, target_language)\n",
        "            full_transcription = translate_text(full_transcription, target_language)\n",
        "\n",
        "        # Cleanup\n",
        "        os.remove(audio_file_path)\n",
        "        for chunk in chunks:\n",
        "            os.remove(chunk)\n",
        "\n",
        "        print(len(full_summary))\n",
        "\n",
        "\n",
        "        return jsonify({\n",
        "            \"summary\": full_summary,\n",
        "            \"transcription\": full_transcription,\n",
        "            \"mind_map\": keyword_sentences,\n",
        "            \"central\": most_frequent_word\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    port = 5000\n",
        "    public_url = ngrok.connect(port).public_url\n",
        "    print(\"Public URL:\", public_url)\n",
        "    app.run(host=\"0.0.0.0\",port=port)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
